{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2616705e",
   "metadata": {},
   "source": [
    "# Introduction to TF-IDF\n",
    "While calculating the most frequent words in a text can be useful, the most frequent words in a text usually aren’t the most interesting words in a text, even if we get rid of stop words. TF-IDF is a method that builds off word frequency but it more specifically tries to identify the most distinctively frequent or significant words in a document.<br><br>\n",
    "TF-IDF = term_frequency * inverse_document_frequency<br>\n",
    "term_frequency = number of times a given term appears in document<br>\n",
    "inverse_document_frequency = log(total number of documents / number of documents with term) + 1<br><br>\n",
    "The reason we take the inverse, or flipped fraction, of document frequency is to boost the rarer words that occur in relatively few documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f29d03",
   "metadata": {},
   "source": [
    "# TF-IDF: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dealing with dataframe\n",
    "import json # dealing with json datafiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6c65e",
   "metadata": {},
   "source": [
    "The books is available at: https://babel.hathitrust.org/cgi/pt?id=loc.ark:/13960/t6737fd9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json datafile\n",
    "file_path = 'Data/loc.ark+=13960=t6737fd9d.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency data of each page\n",
    "data['features']['pages'][60]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acec2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the frequency of each word in each page to a dataframe\n",
    "page_list = [] # empty lists to record the information\n",
    "token_list = []\n",
    "count_list = []\n",
    "for i in range(len(data['features']['pages'])): # loop through each page\n",
    "    if data['features']['pages'][i]['body'] is not None: # if that page has word frequency information\n",
    "        for token in data['features']['pages'][i]['body']['tokenPosCount']: # loop through each word\n",
    "            token_count = 0\n",
    "            for pos_keys in data['features']['pages'][i]['body']['tokenPosCount'][token]: # add up the total occurences of that word\n",
    "                token_count += data['features']['pages'][i]['body']['tokenPosCount'][token][pos_keys]\n",
    "            page_list.append(i+1) # add one to page number because there is no page 0\n",
    "            token_list.append(token) # add the word\n",
    "            count_list.append(token_count) # add the frequency of the word\n",
    "word_count_by_page = pd.DataFrame({\n",
    "    'Page': page_list,\n",
    "    'Token': token_list,\n",
    "    'Count': count_list \n",
    "}) # save the data to a dataframe\n",
    "word_count_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26274c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group words into lower cases\n",
    "word_count_by_page = word_count_by_page.groupby([word_count_by_page['Token'].str.lower(), 'Page'])['Count'].sum().reset_index()\n",
    "word_count_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "word_count_by_page = word_count_by_page.drop(word_count_by_page[word_count_by_page['Token'].isin(stop_words)].index).reset_index(drop=True)\n",
    "word_count_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daff99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "word_count_by_page = word_count_by_page.drop(word_count_by_page[word_count_by_page['Token'].str.contains('[^A-Za-z\\s]', regex=True)].index).reset_index(drop=True)\n",
    "word_count_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8662f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add act number information based on page (these boundaries are created based on close examination)\n",
    "def add_act_number(page):\n",
    "    if page >= 53 and page <= 74:\n",
    "        return \"Act I\"\n",
    "    elif page >= 76 and page <= 108:\n",
    "        return \"Act II\"\n",
    "    elif page >= 109 and page <= 139:\n",
    "        return \"Act III\"\n",
    "    elif page >= 140 and page <= 164:\n",
    "        return \"Act IV\"\n",
    "    elif page >= 165 and page <= 179:\n",
    "        return \"Act V\"\n",
    "    \n",
    "word_count_by_page['Act'] = word_count_by_page['Page'].apply(add_act_number)\n",
    "word_count_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b65c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the word frequency in each act only\n",
    "word_count_by_act = word_count_by_page[word_count_by_page['Act'].notna()]\n",
    "word_count_by_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147207f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum word counts for each act\n",
    "word_count_by_act = word_count_by_act.groupby(['Act', 'Token'])[['Count']].sum().reset_index()\n",
    "word_count_by_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b543466",
   "metadata": {},
   "source": [
    "# TF-IDF: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f25969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns so that they’re consistent with the TF-IDF vocabulary that we’ve been using\n",
    "word_frequency_df  = word_count_by_act.rename(columns={'Token': 'term', 'Count': 'term_frequency'})\n",
    "word_frequency_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate DataFrame by adding up how many acts each term appears\n",
    "document_frequency_df = (word_frequency_df.groupby(['Act', 'term']).size().unstack()).sum().reset_index()\n",
    "document_frequency_df = document_frequency_df.rename(columns={0:'document_frequency'})\n",
    "document_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d31b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes together, so that for each term in each act, we got its term frequency in that act, and how many acts\n",
    "# the term appears in the whole play\n",
    "word_frequency_df = word_frequency_df.merge(document_frequency_df)\n",
    "word_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf019319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of acts for inverse document frequency\n",
    "total_number_of_acts = word_frequency_df['Act'].nunique()\n",
    "total_number_of_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17690c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inverse document frequency\n",
    "import numpy as np # performing calculations on arrays\n",
    "word_frequency_df['idf'] = np.log((total_number_of_acts) / (word_frequency_df['document_frequency'])) + 1\n",
    "word_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF scores\n",
    "word_frequency_df['tfidf'] = word_frequency_df['term_frequency'] * word_frequency_df['idf']\n",
    "word_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe to get top 5 words with highest TF-IDF scores in each act\n",
    "word_frequency_df.sort_values(by=['Act', 'tfidf'], ascending=[True,False]).groupby(['Act']).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08ea61",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e024dc8",
   "metadata": {},
   "source": [
    "Examine the top 5 words based on TF-IDF score for each act. Refer to the definition of TF-IDF and address the following questions: (1) What characteristics must a word possess to become a \"top word\" with the highest TF-IDF score? (2) How does a TF-IDF score differ from raw word frequency? (3) Which types of words would become top words when using raw word frequency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d373aa6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "447fa076",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4908f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the top one word with the highest TF-IDF score for each page (instead of act).\n",
    "# Use \"word_count_by_page\" dataframe and copy the codes to produce the word frequency for each page,\n",
    "# then generate the document frequency of each word, merge the two dataframes, calculate the total number of pages,\n",
    "# compute IDF, TF-IDF, and finally, sort by Page and TF-IDF. Group by \"Page\" and select the top 1 word for each page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7f815",
   "metadata": {},
   "source": [
    "# TF-IDF with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of US inaugural addresses\n",
    "US_inaugural = pd.read_csv('Data/US_Inaugural_Addresses.csv')\n",
    "US_inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9068fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer, using English stopwords and converting words to lowercase\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a666996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe of tfidf values using TfidfVectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(US_inaugural['Text']) # Generate a matrix\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()) # Convert matrix to dataframe\n",
    "tfidf_df.set_index(US_inaugural['Title'], inplace=True) # Replace the index to be the name of the inaugural speeches\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize the DataFrame so that the words are in rows rather than columns\n",
    "tfidf_df.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8429309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word with highest TF-IDF score in each inaugural address\n",
    "tfidf_df = tfidf_df.stack().reset_index()\n",
    "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'Title': 'document','level_1': 'term'})\n",
    "tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f52cc",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabae66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the top one word with the highest TF-IDF score for each inaugural address without including stopwords.\n",
    "# Define a new tfidfvectorizer without including stopwords, and copy the codes with that new tfidfvectorizer to fit_transform,\n",
    "# convert the generated matrix to a DataFrame, set the index as title of the address, and reorganize the dataframe,\n",
    "# then rename the columns, and finally sort values and select the top 1 word of each inaugural address\n",
    "# (Don't be surprised if you find they are mostly the same word. That's why stop words removal is important!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e349e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for declarative statistical visualization\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221706c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fancy visualizations to highlight the words with highest TF-IDF score in each inaugural address\n",
    "import altair as alt\n",
    "\n",
    "top_tfidf = tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "\n",
    "# Terms in this list will get a red dot in the visualization\n",
    "term_list = ['war', 'peace']\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_tfidf.copy()\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'document:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"document\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# red circle over terms in above list\n",
    "circle = base.mark_circle(size=100).encode(\n",
    "    color = alt.condition(\n",
    "        alt.FieldOneOfPredicate(field='term', oneOf=term_list),\n",
    "        alt.value('red'),\n",
    "        alt.value('#FFFFFF00')        \n",
    "    )\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle').encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + circle + text).properties(width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b6bff",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79504582",
   "metadata": {},
   "source": [
    "Based on our explorations of TF-IDF scores, address the following questions: (1) What limitations do you think the TF-IDF method has? (2) Can you suggest another potential application for this method? Please provide an example from either academic research or real-life situations and explain the advantages that TF-IDF calculation could offer in that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e81b4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f42bf8",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834c67c",
   "metadata": {},
   "source": [
    "We use VADER for sentiment analysis, which stands for Valence Aware Dictionary and sEntiment Reasoner, calculates the sentiment of texts by referring to a lexicon of words that have been assigned sentiment scores as well as by using a handful of simple rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VADER Sentiment analysis\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER so we can use it later\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ecb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentAnalyser.polarity_scores(\"I like sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentAnalyser.polarity_scores(\"I don't like sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b12d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment score for a text\n",
    "def calculate_sentiment(text):\n",
    "    # Run VADER on the text\n",
    "    scores = sentimentAnalyser.polarity_scores(text)\n",
    "    # Extract the compound score\n",
    "    compound_score = scores['compound']\n",
    "    # Return compound score\n",
    "    return compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437de8e0",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f708046",
   "metadata": {},
   "source": [
    "Calculate the sentiment score of Trump's tweets by applying the \"calculate_sentiment\" function to the \"text\" column of the \"trump\" dataframe. Afterward, print the texts and sentiment scores for the first five tweets in the dataframe. Do the sentiment scores align with your understanding of the text? Keep in mind that the sentiment score ranges from -1 (totally negative) to 1 (totally positive).<br>\n",
    "Include your codes in the following cell (code) and the discussion in the next cell (markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce12f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = pd.read_csv(\"Data/trump.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec820dba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
